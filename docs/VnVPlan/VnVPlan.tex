\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
November 1 & 1.0 & Initial documentation\\
% Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

% ~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables


\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{center}
  \begin{longtable}{| p{0.475\linewidth} | p{0.475\linewidth} |}
  \hline
  \textbf{Symbol/Abbreviation/Acronym} & \textbf{Description} \\
  \hline
  V\&V & Verification and Validation \\
  \hline
  NLP & Natural Language Processing \\
  \hline
  AWS & Amazon Web Services \\
  \hline
  CI/CD & Continuous Integration/Continuous Deployment \\
  \hline
  API & Application Programming Interface \\
  \hline
  SRS & Software Requirements Specification \\
  \hline
  UI & User Interface \\
  \hline
  MG & Module Guide \\
  \hline
  MIS & Module Interface Specification \\
  \hline
  PIPEDA & Personal Information Protection and Electronic Documents Act \\
  \hline
  FIPPA & Freedom of Information and Protection of Privacy Act \\
  \hline

  \caption{Symbols, Abbreviations, and Acronyms}
  \end{longtable}
  \end{center}

\newpage

\pagenumbering{arabic}

% This document ... \wss{provide an introductory blurb and roadmap of the
%   Verification and Validation plan}
This document outlines the strategies and processes used to ensure that the plagiarism detection system  developed by the SyntaxSentinals team meets all functional 
and non-functional requirements. The primary goal of this plan is to build confidence in the correctness, usability, and performance of the system. It also focuses 
on identifying and mitigating potential risks, ensuring that the final product aligns with academic and competition standards.
This document is organized as follows. The general information section provides an overview of the objectives, challenges, and relevant project documents 
used throughout the V\&V process. The plan section describes the roles and responsibilities of the team members and the tools used for automated testing and verification. 
The system tests section lists the tests performed for both functional and non-functional requirements, with traceability to the SRS. 
The unit test description section details the unit testing scope, the modules tested, and the strategies for covering edge cases. Finally, the appendix contains symbolic parameters, 
survey questions (if applicable), and any other relevant information to support the V\&V process. This plan will evolve as the project progresses, with updates following the completion 
of the detailed design and implementation phases.

\section{General Information}

\subsection{Summary}

% \wss{Say what software is being tested.  Give its name and a brief overview of
%   its general functions.}
  The software being tested is a plagiarism detection system designed to identify similarities in Python code submissions, called SyntaxSentinels. 
  This system utilizes natural language processing (NLP) techniques to analyze code semantics, preventing common circumvention methods 
  like adding benign lines or altering variable names. Its core function is to allow users to input code snippets and receive a plagiarism 
  report containing similarity scores, which, when compared to a threshold, indicate the likelihood of plagiarism. This tool is primarily 
  intended for use in academic and competitive environments to promote fairness and integrity in code submissions.

\subsection{Objectives}

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}
The primary objectives of this V\&V plan are to:
\begin{itemize}
  \item Build confidence in the programs correctness by ensuring alignment with the SRS requirements.
  \item Show that we have met the documented safety and security requirements (SR-SAF1- SR-SAF5) in the Hazard Analysis document.
  \item Demonstrate adequate usability in the program by conducting functional and non-functional tests mentioned in this document.
\end{itemize}

Out of Scope:
\begin{itemize}
  \item Validation of any external libraries will be assumed to be handled by their maintainers.
\end{itemize}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

\subsection{Challenge Level and Extras}

% \wss{State the challenge level (advanced, general, basic) for your project.
% Your challenge level should exactly match what is included in your problem
% statement.  This should be the challenge level agreed on between you and the
% course instructor.  You can use a pull request to update your challenge level
% (in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
% VnV planning exercise.}

% \wss{Summarize the extras (if any) that were tackled by this project.  Extras
% can include usability testing, code walkthroughs, user documentation, formal
% proof, GenderMag personas, Design Thinking, etc.  Extras should have already
% been approved by the course instructor as included in your problem statement.
% You can use a pull request to update your extras (in TeamComposition.csv or
% Repos.csv) if your plan changes as a result of the VnV planning exercise.}

This project has been classified as having a General difficulty level, as agreed with the course instructor. 
Planned extras include:
\begin{itemize}
  \item A user manual for instructors and administrators.
  \item Benchmarking of the tool’s effectiveness compared to MOSS.
\end{itemize}

\subsection{Relevant Documentation}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.  You can create BibTeX entries for your
%   documents and within those entries include a hyperlink to the documents.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

The following documents are critical to the development and V\&V efforts for this project.

\begin{itemize}
    \item \textbf{Software Requirements Specification (SRS)}: Defines the project’s requirements, guiding both verification and validation. ~\citep{SRS}.
    \item \textbf{User Guide}: Provides operational instructions, relevant for usability testing. ~\citep{UserGuide}.
    \item \textbf{Module Guide (MG)}: Outlines the system’s architecture, essential for design verification. ~\citep{MG}.
    \item \textbf{Module Interface Specification (MIS)}: Details the internal modules and interfaces, critical for unit testing. ~\citep{MIS}.
    \item \textbf{Hazard Analysis}: Identifies potential risks, guiding validation efforts for safety and security. ~\citep{HazardAnalysis}.
\end{itemize}

\section{Plan}

This section outlines the structured plan for verification and validation of the project. It defines team roles, describes approaches for verifying different phases of the development process, and specifies tools to be used in ensuring the project meets quality standards. The subsections cover SRS verification, design verification, implementation verification, automated testing tools, and software validation.

\subsection{Verification and Validation Team}

The following table lists team members and their respective roles in the verification and validation process. Each member will be responsible for writing test cases, executing tests, and documenting results for their assigned areas. The team will meet regularly to discuss progress, address issues, and ensure alignment with project goals.

\begin{table}[h!]
  \centering
  \begin{tabular}{|p{5cm}|p{10cm}|}
      \hline
      \textbf{Team Member} & \textbf{Role in Verification and Validation} \\
      \hline
      Mohammad Mohsin Khan & Oversees system architecture verification and leads checklist creation. \\
      \hline
      Lucas Chen & Security verification, focusing on access control and data flow assessments. \\
      \hline
      Dennis Fong & Responsible for interface and compatibility reviews between components. \\
      \hline
      Julian Cecchini & Ensuring that individual modules or components integrate smoothly with one another. \\
      \hline
      Luigi Quattrociocchi & Tracks checklist items and maintains verification documentation. \\
      \hline
  \end{tabular}
  \caption{Verification and Validation Team Roles}
  \label{tab:team-roles}
\end{table}


\subsection{SRS Verification Plan}

The SRS (Software Requirements Specification) verification will ensure that all functional and non-functional requirements are accurately documented and align with the project goals. The verification plan includes:

\begin{itemize}
    \item \textbf{Structured Reviews}: Team members will perform a structured review of the SRS document, verifying that all requirements are feasible and testable.
    \item \textbf{Checklist-Based Verification}: An SRS checklist will be used to ensure all critical elements, such as functional completeness and clarity, are covered.
    \item \textbf{Reviewer Feedback Sessions}: We will gather feedback from peer reviewers and our project supervisor, meeting to discuss any discrepancies or ambiguous requirements. These meetings will include task-based inspections, where reviewers are asked to analyze requirements based on specific scenarios.
\end{itemize}

\pagebreak

The checklist for SRS review will cover:

\begin{center}
  \begin{longtable}{|p{4cm}|p{11cm}|}
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endfirsthead
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endhead
  
  \textbf{1. Completeness} & \\
  \hline
  1.1 Purpose and Scope & Document states the purpose and scope of the project. \\
  \hline
  1.2 Stakeholders & Key stakeholders (clients, users) are defined and are relevant. \\
  \hline
  1.3 Functional Requirements & All primary functions (e.g., plagiarism detection, reporting) are covered and are descriptive. \\
  \hline
  1.4 Non-Functional Requirements & Includes performance, usability, and security requirements. \\
  \hline
  \textbf{2. Clarity} & \\
  \hline
  2.1 Unambiguous Terminology & Each requirement is clearly stated, terms are defined (e.g., MOSS, NLP). \\
  \hline
  2.2 Glossary Completeness & All acronyms and terms are included in the glossary. \\
  \hline
  \textbf{3. Consistency} & \\
  \hline
  3.1 Consistent Terminology & Terminology and references are consistent throughout. \\
  \hline
  3.2 No Conflicting Requirements & No contradictory requirements (e.g., conflicting performance vs. security). \\
  \hline
  \textbf{4. Verifiability} & \\
  \hline
  4.1 Testable Requirements & Each `requirement' is testable and verifiable (e.g., accuracy metrics, response times). \\
  \hline
  4.2 Acceptance Criteria & Clear acceptance criteria for each requirement. \\
  \hline
  \textbf{5. Traceability} & \\
  \hline
  5.1 Unique Identifiers & Each requirement has a unique identifier. \\
  \hline
  5.2 Source of Requirements & Requirements link to stakeholder needs or project goals. \\
  \hline
  \textbf{6. Feasibility} & \\
  \hline
  6.1 Technical Feasibility & Requirements are achievable within project constraints. \\
  \hline
  6.2 Practical Constraints & Constraints such as budget and timeline are realistic. \\
  \hline
  \textbf{7. Security and Privacy} & \\
  \hline
  7.1 Data Retention Policy & Compliance with data privacy laws (e.g., PIPEDA). \\
  \hline
  7.2 Access Control Requirements & Requirements for user authentication and authorization are clear. \\
  \hline
  \textbf{8. Modifiability} & \\
  \hline
  8.1 Organized Structure & Requirements are logically organized for easy updates. \\
  \hline
  8.2 No Redundancies & No duplicate requirements to avoid confusion. \\
  \hline
  \textbf{9. Compliance and Ethics} & \\
  \hline
  9.1 Legal and Ethical Standards & Legal (e.g., Copyright) and ethical considerations are addressed. \\
  \hline
  \caption{Checklist for SRS Verification}
  \end{longtable}
  \end{center}

\subsection{Design Verification Plan}

The design verification plan aims to ensure that the design accurately implements the requirements and adheres to best practices. This plan includes:

\begin{itemize}
    \item \textbf{Checklist-Based Design Review}: A checklist will be used to guide reviews, focusing on key aspects such as modularity, scalability, and security.
    \item \textbf{Peer Design Reviews}: Peer reviews by classmates and team members will provide feedback on the design, highlighting potential areas of improvement.
    \item \textbf{Regular Team Reviews}: Scheduled meetings will allow the team to discuss any design modifications, verify alignment with the SRS, and ensure consistency across components.
\end{itemize}

The checklist for design review will cover:
\begin{center}
  \begin{longtable}{|p{4cm}|p{11cm}|}
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endfirsthead
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endhead
  
  \textbf{1. Functional Verification} & \\
  \hline
  1.1 Requirement Mapping & Each design element corresponds to at least one requirement in the SRS. \\
  \hline
  1.2 Functionality Coverage & The design covers all specified functionalities, including error handling and edge cases. \\
  \hline
  1.3 Interface Definition & All interfaces between components are clearly defined and consistent with requirements. \\
  \hline
  
  \textbf{2. Structural Verification} & \\
  \hline
  2.1 Modular Design & The design is divided into logical, independent modules with well-defined interfaces. \\
  \hline
  2.2 Dependency Analysis & Dependencies between components are minimized, and unnecessary couplings are avoided. \\
  \hline
  2.3 Hierarchical Structure & The design follows a clear hierarchy, with higher-level components orchestrating lower-level ones. \\
  \hline
  
  \textbf{3. Usability and Accessibility} & \\
  \hline
  3.1 User Interface Design & UI elements are consistent, intuitive, and meet accessibility standards (if applicable). \\
  \hline
  3.2 Navigation and Flow & User navigation and workflow are logical, efficient, and follow a coherent path. \\
  \hline
  3.3 Accessibility Standards & The design adheres to relevant accessibility guidelines, such as WCAG, to ensure usability for all users. \\
  \hline
  
  \textbf{4. Performance and Optimization} & \\
  \hline
  4.1 Performance Criteria & The design incorporates mechanisms to meet performance requirements (e.g., response time, resource usage). \\
  \hline
  4.2 Scalability & Design supports scalability to handle expected load increases without significant degradation. \\
  \hline
  
  \textbf{5. Security and Privacy} & \\
  \hline
  5.1 Data Flow Security & The design ensures secure data handling, storage, and transmission between components. \\
  \hline
  5.2 Access Control Mechanisms & Roles and permissions are implemented to restrict unauthorized access to sensitive components. \\
  \hline
  5.3 Compliance & Design complies with security and privacy standards as outlined in the SRS. \\
  \hline
  
  \textbf{6. Traceability and Documentation} & \\
  \hline
  6.2 Documentation Completeness & Documentation is complete, with descriptions of components, workflows, and data flow. \\
  \hline
  6.3 Version Control & Design documentation is version-controlled to track changes and updates. \\
  \hline
  \caption{Checklist for Design Verification}
  \end{longtable}
  \end{center}

\subsection{Verification and Validation Plan Verification Plan}

The verification and validation plan itself will also undergo verification to ensure its effectiveness. This will be achieved by:

\begin{itemize}
    \item \textbf{Peer Reviews}: Classmates will review the plan to provide feedback on its clarity, feasibility, and alignment with project requirements.
    \item \textbf{Mutation Testing}: We will apply mutation testing to validate that the plan can effectively catch errors and discrepancies in project requirements and implementation.
    \item \textbf{Checklist-Based Verification}: A checklist specific to the verification and validation plan will guide reviewers in assessing all critical aspects of the plan.
\end{itemize}

\begin{center}
  \begin{longtable}{|>{\centering\arraybackslash}p{4cm}|p{10cm}|}
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endfirsthead
  
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endhead
  
  1. Verification Plan Completeness & Verify that the Verification and Validation Plan includes all necessary sections, such as objectives, scope, and methodologies. \\
  \hline
  2. Clarity of Objectives & Ensure that the objectives of the verification and validation activities are clearly stated and aligned with project goals. \\
  \hline
  3. Methodology Definition & Check that each verification method (e.g., reviews, inspections, testing) is well-defined with clear procedures. \\
  \hline
  4. Team Roles and Responsibilities & Confirm that each team member's role in the verification and validation activities is documented and clear. \\
  \hline
  5. Review and Inspection Procedures & Validate that there are structured procedures for design reviews and inspections, with criteria for passing/failing. \\
  \hline
  6. Integration of Automated Tools & Verify that automated tools for testing and validation (e.g., CI/CD, linters, static analyzers) are specified and included in the plan. \\
  \hline
  7. Traceability of Requirements & Confirm that the verification and validation activities trace back to specific project requirements to ensure coverage. \\
  \hline
  8. Acceptance Criteria & Ensure there are clear acceptance criteria defined for each verification and validation task. \\
  \hline
  9. Documentation of Test Cases & Verify that each planned test case has clear documentation, including expected outcomes, inputs, and procedures. \\
  \hline
  10. Risk Management in Verification & Confirm that potential risks in the verification and validation process are identified and mitigation strategies are documented. \\
  \hline
  11. Feedback Loop & Ensure that there is a mechanism for capturing feedback and iterating on the verification and validation process as needed. \\
  \hline
  12. Reporting and Tracking of Issues & Check that there is a process for documenting, tracking, and addressing issues found during verification and validation. \\
  \hline
  13. Schedule and Milestones & Verify that there is a realistic schedule and milestones for completing verification and validation activities. \\
  \hline
  \caption{Checklist for V\&V Plan Verification}
  \end{longtable}
  \end{center}

\subsection{Implementation Verification Plan}

The implementation verification plan focuses on ensuring the correctness and quality of the implementation phase. The plan includes:

\begin{itemize}
    \item \textbf{Unit Testing}: Each function and module will undergo unit testing using the frameworks mentioned in section 3.6 to ensure they meet functional requirements.
    \item \textbf{Static Code Analysis}: We will use static analysis tools mentioned in section 3.6 to verify code quality, adherence to coding standards, and security practices.
    \item \textbf{Code Walkthroughs}: Code walkthroughs will be held in team meetings and during the final presentation, allowing team members to inspect each other’s code for issues in logic, structure, and readability.
\end{itemize}

\subsection{Automated Testing and Verification Tools}

To streamline the verification process, the following automated testing and verification tools will be used:

\begin{itemize}
  \item \textbf{Testing Framework}: A testing framework (Pytest for Python) and (Playwright for ReactJs) will be used to automate testing of individual functions, modules, and E2E.
  \item \textbf{Continuous Integration (CI) Tool}: GitHub Actions will be set up for continuous integration to ensure that tests are automatically run on new code submissions.
  \item \textbf{Code Coverage Tool}: Code coverage tools (e.g Pytest with Coverage.py) will track the extent to which the codebase has been tested, ensuring that all critical paths are covered.
  \item \textbf{Linters/Static Analysis Tools}: Linters appropriate to the project’s programming language will enforce coding standards, improving code readability and maintainability.
\end{itemize}

\subsection{Software Validation Plan}

The software validation plan outlines the strategies for validating that the software meets the intended requirements. This includes:

\begin{itemize}
    \item \textbf{User Review Sessions}: Review sessions with stakeholders and user representatives will be conducted to validate that the system meets user needs and expectations.
    \item \textbf{Rev 0 Demonstration}: Shortly after the scheduled Rev 0 demo, we will seek feedback from stakeholders and supervisors, if applicable, to confirm that the design and initial implementation align with project goals.
    \item \textbf{End to End Testing}: We will plan E2E sessions to ensure the software works correctly from start to end and meets all functional requirements.
    \item \textbf{Performance Testing}: Once functional requirements have been implemented, non-functional requirements will be implemented then performance testing will be conducted to validate that the software meets non-functional requirements, such as response times and resource usage.
\end{itemize}

The validation process will involve gathering external data, where possible, to test the system’s accuracy and performance under realistic scenarios.

\section*{Summary}

This comprehensive plan for verification and validation addresses each phase of the project lifecycle, from requirements to implementation. By following a structured approach with well-defined roles, checklists, and automated tools, we aim to ensure a high level of quality, accuracy, and security in the final product.

\section{System Tests}

% \wss{There should be text between all headings, even if it is just a roadmap of
% the contents of the subsections.}

This section outlines the system tests to be performed for both functional and non-functional requirements. 
The tests are categorized based on the area of testing, such as look and feel requirements, usability, and versioning. 
Each test includes a title, test ID, control, initial state, input, output, test case derivation, and how the test will be performed.

\subsection{Tests for Functional Requirements}

Each test section addresses a particular initial condition and input set. 
Therefore, each test has a particular set of outputs that the system should 
produce for it. A functional requirement of section 9 in the SRS \citep{SRS}
is only associated with a test if the output set of the test corresponds to 
the fit criterion of that functional requirement, and the fit criterion should 
be an observable component of a system state, such as a display of a return 
item or notification presented. Through this association, every functional 
requirement is provided a test that is capable of verifying it, thereby ensuring 
the test areas fully cover the functional requirements.

\subsubsection{Plagiarism Analysis Input Upload Tests}

Applies to all FRs involving uploading code snippets to the system. This currently 
covers FR-1 in subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-1\\}

Control: Automatic.
					
Initial State: system is active and set up in spot to receive code snippet 
uploads, zero or more existing snippets are currently uploaded.
					
Input: code snippet file(s).
					
Output: System signifies input was received and continues activity without 
error, awaiting further action directives (such as uploading more files or 
initiating analysis).

Test Case Derivation: The system will look to continually receive code 
snippets from user as part of a necessary step to initiate plagiarism analysis.
					
How test will be performed: A script will open the system in the appropriate spot for 
receiving code and will upload files in varying amounts, one batch at a time, all the 
while inspecting for failure during the process. Script will be integrated into CI/CD
pipeline to ensure continued system functionality (types: dynamic, functional).
					

\end{enumerate}

\subsubsection{Plagiarism Analysis Result Tests}

Applies to all FRs involving immediate output of plagiarism analysis. This 
currently covers FR-2, FR-4, and FR-5 in subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-2\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for initiating analysis, 
2 or more code snippets have been uploaded.
					
Input: command to initiate analysis (button, enter, etc.).
					
Output: System presents list of similarity scores for all code snippet pairings 
along with corresponding threshold scores and any code snippet pairing that 
has exceeded its threshold has been flagged. System remains active, 
awaiting for action directives on what to do.

Test Case Derivation: The system should possess an algorithm that analyzes code 
snippets and produces similarity scores and threshold scores in return. These 
scores should be produced between every pairing of code snippets as the 
plagiarism is a relative assessment between one code piece and another. The 
system must able to pass this algorithm received code snippets and use its
results to flag any pairing that exceeded its threshold before returning from
its analysis state to the user for further interaction.

How test will be performed: A script will open the system's spot to initiate analysis 
with 2 or more code snippets already inserted into the system, and command for 
starting analysis will be entered. Errors will be inspected for 
up until similarity scores and thresholds have been presented. By this point,
code flaggings should also be available to ascertain. (type: dynamic, functional).
					

\end{enumerate}

\subsubsection{Guide Documentation Generation Tests}

Applies to all FRs involving guide documentation generation for user. This 
currently covers FR-3 in subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-3\\}

Control: Automatic.
					
Initial State: System is active and set up in spot for generating guide 
documentation.
					
Input: command for documentation presentation (button, enter, etc.).
					
Output: System generates documentation in a viewable medium, such as text or a 
PDF file, and remains active while awaiting for further action directives.

Test Case Derivation: system possesses guide documentation which it should be 
capable of transferring to user to provide guidance on use cases of system 
when prompted.

How test will be performed: A script will open the system's spot for generating 
guide documentation and command for guide generating documentation will be 
entered, all the while inspecting for error. It will check for missing 
documentation at the end. (type: dynamic, functional).				

\end{enumerate}

\subsubsection{Report Documentation Generation Tests}

Applies to all FRs involving analysis report documentation generation for 
user. This currently covers FR-6 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-4\\}

Control: Automatic.
					
Initial State: System is active and set up in spot for report generation, 
most recent plagiarism analysis has been completed since after system activation 
and its results are available.
					
Input: command for report documentation generation (button, enter, etc.).
					
Output: System provides report documentation in a viewable medium, such
as text or a PDF file, and remains active awaiting for further action 
directives.

Test Case Derivation: The system should possess the ability to aggregate
the results of the plagiarism analysis which has most recently occurred 
and insert them into a template that can summarize all findings to a user,
which will become the report document given.

How test will be performed: A script will have the system conduct a 
plagiarism analysis to provide inputs for the report generation as this
simulates what must happen before a user can unlock report generation. 
If successful, it will proceed to open the spot for generating a report 
where the command for generating a report will be entered, all the while 
inspecting for errors during the process. It will check for missing 
documentation at the end (type: dynamic, functional).			

\end{enumerate}

\subsubsection{Account Creation Tests}

Applies to all FRs involving creating an account within the system. This currently
covers FR-7 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-5\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account creation.
					
Input: command for account creation (button, enter, etc.) alongside account 
user email and password, and the email is not associated with any existing 
account.
					
Output: System indicates account creation was successfully created for 
logging in with (evidenced by existence of email within created account list)
and remains active, awaiting further action directives.

Test Case Derivation: The system should be able to assess a set of account 
credentials is not yet within the system and proceed to add this set to the 
set of accounts that can be logged in with.

How test will be performed: A script will open the system's spot for account creation, 
email and password not associated with any existing account will be given in the 
appropriate area, and command for account creation will be entered; all the while inspecting 
for any failure mode within the process. Created account will be inspected for at the end.
(type: dynamic, functional).

\item{test-FR-6\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account creation.
					
Input: command for account creation (button, enter, etc.) alongside account 
user email and password, and the email is associated with an existing 
account.
					
Output: System notifies account was not possible to create for logging 
in with and remains active, awaiting further action directives.

Test Case Derivation: A set of pre-existing account credentials should not be 
possible to create an account with. Otherwise, account creation is arbitrary 
and not truly provided by the system as any set of account credentials are not 
tied to any particular account.

How test will be performed: A script will open the system's spot for account 
creation, email and password asssociated with an existing account will be 
given in the appropriate area, and command for account creation will be 
entered; all the while inspecting for any failure mode within the process 
(type: dynamic, functional).
					

\end{enumerate}

\subsubsection{Account Login Tests}

Applies to all FRs involving logging into account within the system. This 
currently covers FR-8 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-7\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account 
login.
					
Input: command for account login (button, enter, etc.) alongside account 
user email and password, and the email is associated with an existing 
account.
					
Output: System notifies account login was successful and and remains 
active, awaiting further action directives.

Test Case Derivation: The system should be able to validate a set of 
pre-existing account credentials to facilitate login.

How test will be performed: A script will open the system's spot for account 
login, email and  password asssociated with existing account will be given 
in the appropriate area, and command for account login will be passed; all 
the while inspecting for any failure mode within the process (type: dynamic, 
functional).

\item{test-FR-8\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account login.
					
Input: command for account login (button, enter, etc.) alongside account 
user email and password, and the email is not associated with any existing 
account.
					
Output: System notifies account login was not successful and remains 
active, awaiting further action directives.

Test Case Derivation: A set of account credentials not associated an existing 
account will fail to allow login as the system should determine there is no 
account to validate against.

How test will be performed: A script will open system's spot for account login, 
email and password not asssociated with existing account with will be given in 
the appropriate area, and command for account login will be passed; all the 
while inspecting for any failure mode within the process (type: dynamic, functional).
					
\end{enumerate}

\subsubsection{Result Email Tests}

Applies to all FRs involving emailing results of plagiarism analysis to users. 
This currently covers FR-9 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-9\\}

Control: Automatic.
					
Initial State: System is active and set up in spot for emailing results,
most recent plagiarism analysis has been completed since after system activation 
and its results are available.
					
Input: command for emailing result (button, enter, etc.) and email to 
receive results.
					
Output: System notifies email has been sent and remains active, awaiting 
further action directives. External to system, the specified email shall
contain a .zip file possessing results of the recent analysis.

Test Case Derivation: The system should possess the ability to condense results
into a .zip file upon demand, and it should proceed to carry out emailing this 
file after creating it.

How test will be performed: A script will have the system conduct a 
plagiarism analysis to provide inputs for emailing as this
simulates what must happen for a user before they are able to initiate
the email function. If successful, the script will continue to the system's spot 
for emailing results and the command for sending email will be entered alongside 
an email for receiving the results; all the while inspecting for errors in the 
process. The reception of the .zip file will be assessed at the end once 
the system is awaiting action directives (type: dynamic, functional).
					

\end{enumerate}

\subsubsection{Result Visualization Tests}

Applies to all FRs involving visualizing plagiarism analysis results provided 
by user in a .zip file. This currently covers FR-10 of subsection 9.1 of the SRS 
\citep{SRS}.

\begin{enumerate}

\item{test-FR-10\\}

Control: Automatic
					
Initial State: System is active and set up in the spot for inserting .zip 
files containing results to produce visualization.
					
Input: command for visualization (button, enter, etc.) and .zip file containing 
results.
					
Output: System provides visualization in viewable medium, such as image or file,
that corresponds to the results in the given .zip file.

Test Case Derivation: System should possess ability to parse .zip file for 
relevant contents and pass it into an internal visualization template that 
can be filled out with what was found before proceeding to transfer it to
the user.

How test will be performed: A script will open the system's spot for 
visualization creation and insert a .zip file containing plagiarism results 
before entering the command for creating a visualization. It will inspect for 
any errors during this process. At the end, the script will inspect for 
missing parts of the visualization.
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

% \wss{The nonfunctional requirements for accuracy will likely just reference the
%   appropriate functional tests from above.  The test cases should mention
%   reporting the relative error for these tests.  Not all projects will
%   necessarily have nonfunctional requirements related to accuracy.}

% \wss{For some nonfunctional tests, you won't be setting a target threshold for
% passing the test, but rather describing the experiment you will do to measure
% the quality for different inputs.  For instance, you could measure speed versus
% the problem size.  The output of the test isn't pass/fail, but rather a summary
% table or graph.}

% \wss{Tests related to usability could include conducting a usability test and
%   survey.  The survey will be in the Appendix.}

% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
% format for the tests given below.}

% \wss{If you introduce static tests in your plan, you need to provide details.
% How will they be done?  In cases like code (or document) walkthroughs, who will
% be involved? Be specific.}

This section outlines the system tests to be performed for non-functional requirements, including accuracy, performance, and usability.

\subsubsection{Look and Feel}
These tests are to ensure that the way the front end is designed adheres to the
defined requirements with respect to layout, typography, component alignment,
etc. 

\begin{description}
  \item[Front End Component Review:] We will engage a developer to look through
  the code and interface and ensure components are implemented as they are
  defined in the SRS.
  \item[Accessibility Check:] A developer will review if accessibility tools
  exist or not within the application. Important tools are screen readers and
  alt text for images. Furthermore, the developer must check for sufficient
  colour contrast between components.
\end{description}

\paragraph{Validate Look and Feel User Interface Tests}
\begin{enumerate}

\item{test-LF-1}

NFR: LF-AR1, LF-AR2, LF-AR3, LF-SR1, LF-SR2, LF-SR3, LF-SR4

Type: Manual, Front End Review
					
Initial State: Application on home page, ready for use
					
Input/Condition: Tester engagement
					
Output/Result: List of all identified inconsistencies that are found that do not adhere to SRS defined requirements.
					
How test will be performed: 
\begin{enumerate}
  \item A developer will review the code to make sure the look and feels requirements were implemented:
  \begin{enumerate}
    \item They will check that the application has a a uniform colour palette.
    \item They will check that the application has tooltips on buttons with informative text.
    \item They will check that the application is responsive and scales well for different screen sizes and resolutions.
    \item They will check that the application uses the Roboto font family.
    \item They will check that the application has uniform spacing between elements.
    \item They will check that the application has buttons with rounded corners and change colour upon hover.
  \end{enumerate}
  \item They will then check that the application to ensure that it adheres to all look and feel requirements.
\end{enumerate}

\item{test-LF-2}

NFR: CR-SC3

Type: Manual, Accessibility Check
					
Initial State: Application on home page, ready for use
					
Input: Tester engagement
					
Output: List of missing accessibility considerations that have been overlooked
					
How test will be performed: 
\begin{enumerate}
  \item The developer will review the code and application to ensure that accessibility considerations have been implemented.
  \item They will verify that all images have alt text.
  \item They will verify that all text has sufficient contrast and sizing using Google Chrome devtools.
  \item The developer will note down any missing implementations of accessibility requirements.
\end{enumerate}

\end{enumerate}

\subsubsection{Usability and Humanity}
These test cases are intended to build a foundation for the user's experience
on the application. Thus, the only real way to get feedback the user experience
is to have someone use the application and provide feedback.

\begin{description}
  \item[User Test Demo:] We will engage a user to test the application, bringing
  them through a new user process, and ask for feedback on specific areas
  relevant to the requirements that were set. The feedback will be a result of a
  survey, with questions structured as "On a scale of 1 to 10, how would you
  rate X?" with X being some aspect of usability. 
  
\end{description}

\paragraph{Validate Usability and Humanity tests}
\begin{enumerate}

  \item{test-UH-1}
  
  NFR: UH-E1, UH-E3, UH-L1, UH-UP1, UH-UP2, OE-P1
  
  Type: Manual, User Test Demo
            
  Initial State: Application on home page, ready for use
            
  Input/Condition: User engagement
            
  Output/Result: Subjective feedback for how well application scores on different requirements relating to user experience
            
  How test will be performed: 
  \begin{enumerate}
    \item A user will use the application, going through the onboarding process
    and doing a demo of the application.
    \item After a full demo, the user will be asked to provide feedback on
    different aspects of usability, such as ease of use, app clarity, and prompt
    politeness.
  \end{enumerate}
            
  \item{test-UH-2}
  
  NFR: UH-E2
  
  Type: Manual
            
  Initial State: Application on home page, ready for use
            
  Input: Tester engagement
            
  Output: A count for the number of clicks required to navigate to different functionality
            
  How test will be performed: 
  \begin{enumerate}
    \item The user will navigate to each of the following functions in the user interface from the home page:
    \begin{enumerate}
      \item The user will make a code submission to be analyzed.
      \item The user will upload a generated report file to be viewed.
    \end{enumerate}
    \item The user will note how many clicks were required to accomplish each function.
  \end{enumerate}

  \item{test-UH-3}
  
  NFR: UH-PI2

  Type: Manual
            
  Initial State: Application on home page, ready for use
            
  Input: Tester engagement
            
  Output: All identified instances of foreign languages used
            
  How test will be performed: 
  \begin{enumerate}
    \item The tester will navigate to each page and submenu in the application.
    \item They will verify that all pages and prompts are in the correct language (English (US)).
  \end{enumerate}
  
  \end{enumerate}

\subsubsection{Versioning}
These testcases are for versioning requirements that are key in order 
to roll back to old iterations of the code or old iterations of the model 
when needed. These will mainly be tested by checking if old versions are saved. 
Note that this check will be done after each iteration of the model.

\paragraph{Validate old versions of project}
\begin{enumerate}
  \item{test-V-1}

  NFR: MS-M1, OR-R2
  
  Type: Document Review
            
  Initial State: N/A
            
  Input/Condition: Tester engagement
            
  Output/Result: Historical versions of machine learning models with a changelog 
  should be stored and accessible on GitHub
            
  How test will be performed: 
  \begin{enumerate}
    \item A developer will navigate to the project GitHub repository and find the 
    folder containing all past and current versions of models, each with an 
    associated performance report, as well as a changelog including dates of revisions
    \item This will be done with each iteration of the model code.
  \end{enumerate}
\end{enumerate}

\subsubsection{Privacy/Security}
These testcases ensure that the system is secure, and does not violate any
privacy rights. The tests for these requirements will revolve around doing
audits to ensure system integrity.

\begin{description}
  \item[Data Retention Audit:] We will manually check the file system and any
  logs to look for any user-generated data, or personally identifiable data.
  This audit may also be done in a test environment to check live logs of the
  system.
  \item[Endpoint Security Check:] A tester will invoke backend endpoints and
  ensure that they are secure. They must also check that the website uses https
  encryption (padlock symbol in address bar)
  \item[Legal Audit:] We will manually check the system to ensure that it
  adheres to legal policies defined in the SRS, such as PIPEDA and FIPPA. This
  audit will be done regularly, and with each new version release, to the best
  of our abilities.
\end{description}


\paragraph{Validate Zero Data Retention and data encryption}
\begin{enumerate}
  \item{test-PS-1}
  
  NFR: SR-P1

  Type: Manual, Data Retention Audit

  Initial State: Application ready for use

  Input/Condition: Tester engagement

  Output/Result: A list of all identified user-generated or personally identifiable data

  How test will be performed:
  \begin{enumerate}
    \item A tester will enable logging in the components of the application.
    \item They will do a run through of the different functionalities, including 
    making a submission and analyzing a generated report file.
    \item With each function, the tester will check logs and filesystem, and then
    search for any user-generated or personally identifiable data.
  \end{enumerate}


  \item{test-PS-2}
  
  NFR: SR-P2 

  Type: Dynamic

  Initial State: Application ready for use

  Input/Condition: Backend endpoint invocation

  Output/Result: A list of exposed endpoints that do not support in-transit encryption

  How test will be performed:
  \begin{enumerate}
    \item Script will invoke backend end points using HTTPS protocol.
    \item They will ensure that the response provides relevant security headers.
  \end{enumerate}


  \item{test-PS-3}
  
  NFR: CR-L1, CR-L2, CR-L3, CR-S2, CR-S5

  Type: Manual, Legal Audit

  Initial State: Application ready for use

  Input/Condition: Tester engagement
  
  Output/Result: List of components of the system that breach any regulations, 
  with an accompanying explanation of the how the breach could manifest itself

  How test will be performed:
  \begin{enumerate}
    \item A third party tester will inspect the code repository and system 
    application against laws and regulations, noting any component of the system 
    that may potentially infringe upon any data or privacy laws, intellectual 
    property, or academic integrity.
  \end{enumerate}
\end{enumerate}

\subsubsection{Development Standards}
These requirements ensure that the system is built in a way that adheres to
standard software development principles. This ensures that the code is
maintainble, scalable, and reliable.

\begin{description}
  \item[Code Inspection:] The code will be inspected to make sure the system is
  built adhering to software development standards.
  \item[Respository Inspection:] The Github repository for this project will be
  checked to make sure bugs are listed as issues to keep track of what needs to
  be done in the system.
\end{description}

\paragraph{Validate Development Standards}
\begin{enumerate}
  \item{test-DS-1}
  
  NFR: MS-M3

  Type: Manual, Repository Inspection

  Initial State: Development temporarily paused

  Output/Result: Indication of bugs being tracked on Github

  How test will be performed:
  \begin{enumerate}
    \item A tester will look at the 'issues' section of the github repository.
    \item They will check that there are bugs that exist as issues.
    \item If there aren't, they will consult the team and ask if developers are
    issuing bugs or if there are simply no bugs.
  \end{enumerate}


  \item{test-DS-2}
  
  NFR: MS-S1, CR-SC1

  Type: Manual, Code Inspection

  Initial State: Development temporarily paused

  Output/Result: Indication of components not adhering to software development standards

  How test will be performed:
  \begin{enumerate}
    \item Team members will regularly inspect the code and ensure all components 
    follow software development standards, including SOLID principles and the 
    existence of documentation and tests.
    \item If there are components that don't they will consult the other team 
    members to discuss issues with that component.
  \end{enumerate}
\end{enumerate}

\subsubsection{Documentation}
This section is to ensure that documentation exists to guide users and help them 
troubleshoot issues they are having.

\begin{description}
  \item[Documentation Review:] A tester will look through the webpage, and
  ensure that documentation exists. Then, they must check to make sure the
  documentation is accurate, and not misleading. 
\end{description}

\paragraph{Validate Program, help, and training documentation exists and is accurate}
\begin{enumerate}
  \item{test-D-1}
  
  NFR: UH-L2, OE-P2

  Type: Manual, Documentation Review

  Initial State: Application ready for use

  Output/Result: Certification that documentation is valid

  How test will be performed:
  \begin{enumerate}
    \item A developer will navigate to the user documentation and help pages within the 
    application. They must first ensure that documentation exists, and is easy to find.
    \item They must then verify that the documentation is correct, complete, uses clear
     non-technical language, and is helpful for users.
  \end{enumerate}
\end{enumerate}


\subsubsection{Performance}
These testcases ensure that the system is able to handle a certain amount of load, and that it is able to process data in a timely manner.

\paragraph{Batch processing time is less than 4 hours for assumed upper bound of classroom size}

\begin{enumerate}
  \item{test-PR-1}
  
  NFR: PR-SL1, PR-C1

  Type: Performance, Dynamic

  Initial State: The system is idle and ready to process a batch of code submissions

  Input: 500 code snippets of 200 lines or less 

  Output: The system completes processing the batch within 4 hours.

  How test will be performed: Script will open system's spot for plagiarism analysis 
  and pass system 500 code snippets of 200 lines or less alongside command to initiate analysis. 
  It will use a computer timer through a library, such as pyperf, to time the interval between 
  the analysis start and the analysis return. If the interval is under 4 hours, the test will 
  return a pass. Will be integrated into a CI/CD pipeline and be run periodically to affirm 
  system health and performance over time.

  % \begin{enumerate}
  %   \item Submit a batch of code submissions to the system.
  %   \item Measure the time taken for the system to process the batch.
  %   \item Verify that the processing time is less than 600 seconds.
  % \end{enumerate}
\end{enumerate}

\paragraph{Notification is sent when Processing Time Exceeds Ten Minutes}
\begin{enumerate}
  \item{test-PR-2}

  NFR: PR-SL2

  Type: Performance, Manual
            
  Initial State: System is idle; no processes are currently running.
            
  Input/Condition: Start a task that is expected to run for more than ten minutes.
            
  Output/Result: An email is sent to the user once processing exceeds ten minutes.
            
  How test will be performed: 
  \begin{enumerate}
    \item Start a process that is known to take more than ten minutes to complete. For example, a large number of files.
    \item Wait for 10 minutes while the process is running.
    \item Check the user's email inbox for the notification and verify it was received exactly 10 minutes after the process started.
    \item Check that the notification for correctness and that it is clear and understandable for the user.
  \end{enumerate}

  \item{test-PR-3}

  NFR: PR-SL2

  Type: Performance, Dynamic
            
  Initial State: System is idle; no processes are currently running.
            
  Input: Initiate a new task that is simulated to take over ten minutes.
            
  Output: Automated system log entry indicating that a user notification was triggered after ten minutes.
            
  How test will be performed: 
  \begin{enumerate}
    \item {Setup a mock process to run for over 10 minutes.}
    \item {Verify after 10 minutes were up wether a function call was made to send the notification.}
    \item {Automatically check system logs or notifications queue to verify a notification was generated after ten minutes.}
  \end{enumerate}
\end{enumerate}

\paragraph{System remains operational after malformed input}
\begin{enumerate}
  \item{test-PR-4}

  NFR: PR-RFT1

  Type: Robustness, Dynamic
            
  Initial State: System is operational, awaiting user input.
            
  Input:  Malformed input (e.g., corrupted file) is submitted to the system.
            
  Output: System displays an error message to the user indicating invalid input, without crashing or becoming unresponsive.
            
  How test will be performed: 
  \begin{enumerate}
    \item navigate to the home page.
    \item Submit a file that is known to be corrupted or invalid.
    \item Verify that an error message is displayed to the user by checking if the function call was made.
    \item Submit a file that is known to be valid.
    \item Verify that the system accepts the valid input and processes it correctly.
  \end{enumerate}
\end{enumerate}

\paragraph{System meets accuracy expectations}
\begin{enumerate}
  \item{test-PR-5}

  NFR: PR-PA1, PR-PA2

  Type: Performance, Dynamic
            
  Initial State: System is operational and awaiting user input for code submission
            
  Input: Code snippet test set (on the scale of 500 snippets) and command to initiate analysis
            
  Output: All system analysis results have greater than 90\% accuracy and less than 5\%
  false positives compared against the ground truths when using default threshold
            
  How test will be performed: 
  Script will pass current test set of code snippets and give command-line argument to 
  initiate analysis from detector. Upon return of analysis, it will compare results against 
  ground truths of test set using default threshold. If accuracy rate is 90\% and false positives 
  are lower than 5\%, the test will return a pass. This will be integrated into a CI/CD pipeline and 
  be run periodically to affirm system health and performance over time. Also, the test set will potentially
  change over time to keep any possible bias in check.
\end{enumerate}

\paragraph{Feature Updates Do Not Degrade Performance}
\begin{enumerate}
  \item{test-PR-6}

  NFR: PR-SE1

  Type: Functional, Dynamic, Regression
            
  Initial State: N/A
            
  Input: Code for new feature is added 
            
  Output: All base features still function as expected with new feature code in place
            
  How test will be performed: When a new feature is added to the system (as denoted in a 
  pull request or otherwise), all system tests for functional requirements
  are re-ran to ensure no previous functionality has been lost. This will be facilitated
  by a CI/CD pipeline (using GitHub actions) for all functional requirement tests.
\end{enumerate}

\subsubsection{Operational and Environmental}
These testcases ensure that the system is able to operate in different environments and 
under different conditions.

\paragraph{System is able to interface with cloud computing services}
\begin{enumerate}
  \item{test-OE-1}

  NFR: OE-IAS1

  Type: Manual
            
  Initial State: System setup with no cloud service connected.
            
  Input:  User attempts to configure and connect the system to a cloud service (e.g. AWS or Azure).
            
  Output: System successfully connects to the selected cloud service and displays a confirmation message.
            
  How test will be performed: 
  \begin{enumerate}
    \item navigate to the cloud configuration settings.
    \item Select a cloud provider (AWS or Azure) and enter necessary credentials.
    \item Verify that the system displays a confirmation message upon successful connection.
  \end{enumerate}
\end{enumerate}

\paragraph{System is deployable on Free Hosting Service}
\begin{enumerate}
  \item{test-OE-2}

  NFR: OE-IAS2

  Type: Manual
            
  Initial State: User clones the system repository from GitHub.
            
  Input:  Deploy the system on a free hosting service (e.g. Heroku, Netlify).
            
  Output: System is accessible via a public URL and functions as expected.
            
  How test will be performed: 
  \begin{enumerate}
    \item Clone the system repository from GitHub.
    \item Deploy the system on a free hosting service.
    \item Verify that the system is accessible via a public URL and functions as expected without any issues.
  \end{enumerate}
\end{enumerate}

\paragraph{System is able to authenticate the user via external services}
\begin{enumerate}
  \item{test-OE-3}

  NFR: OE-IAS3

  Type: Manual
            
  Initial State: User is not authenticated and does not have a valid auth token.
            
  Input: User attempts to log in using an external service (e.g. Google, GitHub).
            
  Output: System authenticates the user and grants access to the UI.
            
  How test will be performed: 
  \begin{enumerate}
    \item Navigate to the login page.
    \item Select an external service (e.g. Google, GitHub) to log in with.
    \item Verify that the system authenticates the user and is redirected to the home page.
  \end{enumerate}
\end{enumerate}

\subsubsection{Maintainability and Support}
These testcases ensure that the system is maintainable and supportable. This includes ensuring that the system is easy to maintain and that support is available to users.

\paragraph{Model release is accompanied by a report of metrics}

\begin{enumerate}

  \item{test-M-1}

  NFR: MS-M2

  Type: Inspection

  Initial State: The model has been trained and is ready for release

  Input: Release the model

  Output: A report is generated with metrics such as accuracy, precision, etc.

  How test will be performed:
  Release the model and verify that a report is generated with relevant metrics. 
  A verifiable metric is that the report exists.This test will be done manually 
  by the team members of SyntaxSentinels.

\end{enumerate}

\paragraph{A pathway for users to post or vote for requests/issues}

\begin{enumerate}
  \item{test-M-2}

  NFR: MS-S3

  Type: Inspection

  Initial State: The user has a GitHub account and is logged in

  Input: User posts a request or issue

  Output: The request or issue is posted and visible to other users

  How test will be performed:
  The user will post a request or issue and verify that it is visible to other users. 
  A verifiable metric is that the request or issue is visible to other users.
  This test will be done manually by the team members of SyntaxSentinels.
\end{enumerate}

\paragraph{Model Code follows Template }

\begin{enumerate}
  \item{test-M-3}

  NFR: MS-A2

  Type: Inspection, Static

  Initial State: N/A

  Input: Source code pertaining to machine learning model is passed to a parsing script

  Output: The output of the parsing script states model components all follow templates

  How test will be performed:
  Source code pertaining to machine learning model will be statically checked
(through a parsing script looking for reserved class words or ordering potentially) 
to see if model contains components/layers that inherit an interface/class. If all 
components/layers are found to inherit an interface/class, it is assumed it will be simple 
to create or exchange components/layers of the model in code to keep it up with research 
without major overhaul to the model code in its entirety (although there may be significant
changes within individual components/layers). This will be integrated into a CI/CD pipeline and run
periodically to ensure model code remains in adherence to templates.
\end{enumerate}

\paragraph{Model Code follows Template }

\begin{enumerate}
  \item{test-M-4}

  NFR: MS-A1

  Type: Dynamic 

  Initial State: System is inactive
  (cannot modify model during activity)

  Input: None into system directly, training script receives two formats of training sets

  Output: Training script successfully completes epoch 

  How test will be performed:
  Training script for model provided will receive two different formats of 
  training set data from a test script, and test script will verify training 
  occurred either through inspecting a generated checkpoint file or a return 
  value from the training script. This will be integrated into a CI/CD pipeline 
  and run periodically to ensure compatibility for model training remains.
\end{enumerate}

\paragraph{Model Code follows Template }
\begin{enumerate}
  \item{test-M-5}

  NFR: MS-A3

  Type: Manual, Unit

  Initial State: N/A

  Input: A set of appropriately dimensioned vectors into model component/layer

  Output: Expected modification of vector according to component/layer architecture

  How test will be performed:
  Every unique model layer/component will attempt to be separated from any preceding and 
  proceeding layer/components, and will be copied into a test function where it 
  will be given an input vector of appropriate size. The output function should 
  apply transformations strictly associated with that layer and no other, which 
  should be evident through output vector size and contents of the vector. Must 
  be manually done as assessing how to separate layers may not be possible to be 
  done in a consistent manner, even if layers/components are ultimately modular. 
  The difference of having embedded layers that are not modular versus separable 
  layers is akin to having z=f(g(x)) versus y=g(x) and z=g(y). Test must only be done 
  every time a new type of model component or layer is added.
\end{enumerate}

\subsubsection{Security}
These testcases ensure that the system is secure and that user data is protected.

\paragraph{User can access UI with valid login credentials}
\begin{enumerate}
  \item{test-S-1}

  NFR: SR-A1

  Type: Dynamic
            
  Initial State: The user is not authenticated and does not have a valid auth token.
            
  Input/Condition: User enters valid login credentials (username and password).
            
  Output/Result: The user is authenticated and gains access to the UI.
            
  How test will be performed: 
  This test will be done via UI automated test suite using Playwright. The test 
  will simulate a user entering valid login credentials and verify that the user 
  is authenticated and gains access to the UI.
\end{enumerate}
\paragraph{User can access API with valid auth token}
\begin{enumerate}
  \item{test-S-2}

  NFR: SR-A1

  Type: Dynamic
            
  Initial State: The user is not authenticated and does not have a valid auth token
            
  Input: User attempts to access the API with a valid auth token
            
  Output: The API allows the user to upload code for comparison
            
  How test will be performed: 
  Pass a valid auth token to the API and verify that the system allows code upload 
  and returns a success response.

  \item{test-S-3}

  NFR: SR-A1

  Type: Dynamic

  Initial State: The user is not authenticated and does not have a valid auth token

  Input: User attempts to access the API with an invalid or missing auth token

  Output: The API denies access and returns an unauthorized response

  How test will be performed:
  Pass an invalid or missing auth token to the API and verify that the system denies 
  access and returns an unauthorized response.

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{center}
  \begin{longtable}{|>{\centering\arraybackslash}p{4cm}|>{\centering\arraybackslash}p{10cm}|}
  \hline
  \textbf{Test ID} & \textbf{Requirements} \\
  \hline
  \endfirsthead
  
  \hline
  \textbf{Test ID} & \textbf{Requirements} \\
  \hline
  \endhead
  
  test-FR-1 & FR-1 \\
  \hline
  test-FR-2 & FR-2, FR-4, FR-5 \\
  \hline
  test-FR-3 & FR-3 \\
  \hline
  test-FR-4 & FR-6 \\
  \hline
  test-FR-5 & FR-7 \\
  \hline
  test-FR-6 & FR-7 \\
  \hline
  test-FR-7 & FR-8 \\
  \hline
  test-FR-8 & FR-8 \\
  \hline
  test-FR-9 & FR-9 \\
  \hline
  test-FR-10 & FR-10 \\
  \hline
  test-LF-1 & LF-AR1, LF-AR2, LF-AR3, LF-SR1, LF-SR2, LF-SR3, LF-SR4 \\
  \hline
  test-LF-2 & CR-SC3 \\
  \hline
  test-UH-1 & UH-E1, UH-E3, UH-L1, UH-UP1, UH-UP2, OE-P1 \\
  \hline
  test-UH-2 & UH-E2 \\
  \hline
  test-UH-3 & UH-PI2 \\
  \hline
  test-V-1 & MS-M1, OR-R2 \\
  \hline
  test-PS-1 & SR-P1 \\
  \hline
  test-PS-2 & SR-P2 \\
  \hline
  test-PS-3 & CR-L1, CR-L2, CR-L3, CR-S2, CR-S5 \\
  \hline
  test-DS-1 & MS-M3 \\
  \hline
  test-DS-2 & MS-S1, CR-SC1 \\
  \hline
  test-D-1 & UH-L2, OE-P2 \\
  \hline
  test-PR-1 & PR-SL1, PR-C1 \\
  \hline
  test-PR-2 & PR-SL2 \\
  \hline
  test-PR-3 & PR-SL2 \\
  \hline
  test-PR-4 & PR-RFT1 \\
  \hline
  test-PR-5 & PR-PA1, PR-PA2 \\
  \hline
  test-PR-6 & PR-SE1 \\
  \hline
  test-OE-1 & OE-IAS1 \\
  \hline
  test-OE-2 & OE-IAS2 \\
  \hline
  test-OE-3 & OE-IAS3 \\
  \hline
  test-M-1 & MS-M2 \\
  \hline
  test-M-2 & MS-S3 \\
  \hline
  test-M-3 & MS-A2 \\
  \hline
  test-M-4 & MS-A1 \\
  \hline
  test-M-5 & MS-A3 \\
  \hline
  test-S-1 & SR-A1 \\
  \hline
  test-S-2 & SR-A1 \\
  \hline
  test-S-3 & SR-A1 \\
  \hline

  \caption{Traceability Matrix}
  \end{longtable}
  \end{center}

% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

\section{Unit Test Description}
This section will not be filled in until after the MIS document has been completed.
% \wss{This section should not be filled in until after the MIS (detailed design
%   document) has been completed.}

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}  

% \wss{To save space and time, it may be an option to provide less detail in this section.  
% For the unit tests you can potentially layout your testing strategy here.  That is, you 
% can explain how tests will be selected for each module.  For instance, your test building 
% approach could be test cases for each access program, including one test for normal behaviour 
% and as many tests as needed for edge cases.  Rather than create the details of the input 
% and output here, you could point to the unit testing code.  For this to work, you code 
% needs to be well-documented, with meaningful names for all of the tests.}

% \subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 
					
% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 

% \item{...\\}
    
% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}
		
% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input/Condition: 
					
% Output/Result: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters}

Currently, there are no symbolic parameters in the document. It is possible that some will be added in the future, however, 
at this point in time, it is too early to determine what they will be.

\subsection{Usability Survey Questions?}

Currently, there are no usability survey questions in the document. It is possible that some will be added in the future, however,
at this point in time, it is too early to determine what they will be.

\newpage{}
\section*{Appendix --- Reflection}

% \wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 

  While writing this deliverable, our team was able to collaboratively clarify and solidify 
  our understanding of both the functional and non-functional requirements. This process helped us align on 
  specific goals and create comprehensive test plans, which ensures that our testing will effectively verify 
  that all key project requirements are met.

  \item What pain points did you experience during this deliverable, and how
    did you resolve them?

  A significant challenge we faced was the extensive amount of non-functional requirements (NFRs), 
  with over 30 NFRs to address. Each NFR required us to develop a detailed test plan, specifying 
  factors like type (e.g., functional, dynamic, manual), initial state, input/conditions, expected 
  output/results, and test method. While the template helped streamline our process, the sheer volume 
  of NFRs meant the documentation grew quickly, and managing this without sacrificing detail was challenging. 
  We prioritized efficiency by dividing NFRs among team members and holding review sessions to ensure consistent 
  quality and adherence to our testing criteria. This allowed us to maintain clarity without being overwhelmed by 
  the documentation demands.
  One of the main challenges we faced was the large amount of 
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \begin{itemize}
    \item \textbf{Dynamic Testing Knowledge}
    \begin{itemize}
        \item \textbf{Online Courses and Tutorials}: We can learn dynamic testing techniques through structured courses on platforms like Udemy, Coursera, or LinkedIn Learning, which cover both functional and non-functional testing strategies.
        \item \textbf{Hands-on Practice with Sample Projects}: Practicing dynamic testing on sample projects or real scenarios allows us to directly apply the techniques, reinforcing our understanding through application.
    \end{itemize}

    \item \textbf{Static Testing Knowledge}
    \begin{itemize}
        \item \textbf{Tool-Specific Documentation and Tutorials}: Reading the documentation and tutorials for static testing tools (e.g. ESLint) helps us understand how to perform effective static code analysis.
        \item \textbf{Webinars and Workshops}: Many companies offer webinars or workshops on static analysis, often with interactive demos that can help us learn the nuances of static testing.
    \end{itemize}

    \item \textbf{Automated Testing Tools}
    \begin{itemize}
        \item \textbf{Training and Certification Programs}: Courses that cover automation tools (like Pytest) and their integration with CI/CD pipelines provide a solid foundation for us in automated testing.
        \item \textbf{Building a Project with CI/CD Integration}: By implementing automated testing on a project using CI/CD tools like Jenkins or GitHub Actions, we can apply automation skills in real scenarios, enhancing both our tool knowledge and integration capabilities.
    \end{itemize}

    \item \textbf{Security Testing Knowledge}
    \begin{itemize}
        \item \textbf{Cybersecurity and Penetration Testing Courses}: Taking courses on platforms like Cybrary or Coursera offers insights into security testing practices, covering areas like penetration testing, vulnerability assessment, and secure code practices.
        \item \textbf{Practice with Security Testing Tools}: We can use tools like OWASP ZAP or Burp Suite for hands-on security testing, which helps us identify vulnerabilities and ensure code security.
    \end{itemize}

    \item \textbf{Performance Testing Knowledge}
    \begin{itemize}
        \item \textbf{Tool-Specific Training (e.g., JMeter, Locust)}: Learning a performance testing tool through its official documentation, tutorials, or community guides helps us understand load testing, scalability testing, and performance profiling.
        \item \textbf{Performance Testing Workshops or Certifications}: Many organizations provide certifications or workshops focused on performance testing methodologies and tool usage, which provide both theoretical knowledge and practical applications for us.
    \end{itemize}

    \item \textbf{Test Case Management}
    \begin{itemize}
        \item \textbf{Exploring Test Management Tools}: Familiarizing ourselves with tools like TestRail, Zephyr, or Jira for creating, organizing, and tracking test cases helps us manage our testing process efficiently.
        \item \textbf{Learning Best Practices in Test Documentation}: By reading resources or taking tutorials on effective test case design and management, we can develop a structured, comprehensive approach to test case documentation.
    \end{itemize}

  \end{itemize}

  \begin{itemize}
    \item \textbf{Lucas Chen:} Security Testing Knowledge, Performance Testing Knowledge
    \item \textbf{Dennis Fong:} Automated Testing Tools, Security Testing Knowledge
    \item \textbf{Julian Cecchini:} Automated Testing Tools, Static Testing Knowledge
    \item \textbf{Mohammad Mohsin Khan:} Dynamic Testing Knowledge, Performance Testing Knowledge
    \item \textbf{Luigi Quattrociocchi:} Static Testing Knowledge, Test Case Management
  \end{itemize}


  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?

  Approaches for acquiring knowledge or mastering the skill has been identified
  above.
  
  \begin{itemize}
    \item \textbf{Lucas Chen:} 
    \begin{itemize}
        \item \textbf{Security Testing Knowledge}  
        \begin{itemize}
            \item \textbf{Chosen Approach}: YouTube Tutorials and online courses
            \item \textbf{Reason for Choice}: YouTube tutorials and online courses provide a flexible and 
            accessible way to learn security testing concepts and tools. By watching tutorials and taking courses, 
            I can gain a solid understanding of security testing practices and apply them effectively in our project.
        \end{itemize}
    
        \item \textbf{Performance Testing Knowledge}  
        \begin{itemize}
            \item \textbf{Chosen Approach}: Tool-Specific Training (e.g., JMeter, PostMan)
            \item \textbf{Reason for Choice}: Tool-specific training allows me to focus on the performance testing 
            tools that are most relevant to our project. By learning tools like JMeter or PostMan, I can gain practical 
            skills that directly apply to our performance testing requirements.
        \end{itemize}
    \end{itemize}
    \item \textbf{Dennis Fong:}
    \begin{itemize}
      \item \textbf{Automated Testing Tools}
      \begin{itemize}
        \item \textbf{Chosen Approach}: Online courses and textbooks
        \item \textbf{Reason for Choice}: This approach was chosen because of
        the flexibility and convenience where I can learn at my own pace.
        Furthermore, online resources are usually more up to date than textbooks
        that may be old
      \end{itemize}

      \item \textbf{Security Testing Knowledge}  
        \begin{itemize}
            \item \textbf{Chosen Approach}: Youtube videos (theory) and online
            courses
            \item \textbf{Reason for Choice}: For security testing, the theory
            behind the foundations of security are very important. Furthermore,
            online courses also provide the knowledge, and application of these
            concepts to learn more about security testing.
        \end{itemize}
    \end{itemize}
    \item \textbf{Julian Cecchini:} 
    \begin{itemize}
      \item \textbf{Automated Testing Tools}  
      \begin{itemize}
          \item \textbf{Chosen Approach}: Hands-on practice with sample projects
          \item \textbf{Reason for Choice}: I previously examined online tutorials 
          and articles which taught the use of jenkins and github actions. I feel the material
          gave me more comfort with the area but not everything completely snapped in place within
          my mind. Therefore, I think it would be very beneficial to make some arbitrary 
          source code which I can make a minor CI/CD pipeline for to gain a sense for
          all the nitty-gritty details involved within the setup of automated testing.
      \end{itemize}
  
      \item \textbf{Static Testing Knowledge}
      \begin{itemize}
          \item \textbf{Chosen Approach}: Tool-Specific Documentation and Tutorials
          \item \textbf{Reason for Choice}: I have used linters and the like before, but 
          have not explored them in-depth. I would enjoy to not just learn superficial/basic
          stuff that can be found in webinars or youtube videos but to really sink my teeth 
          into niche capabilities of linters which could possibly benefit my capstone or 
          personal projects. Therefore, I feel documentation would be the best route seeing
          that it should contain all the functionality of static analysis tools, including experimental or less commonly known/discussed aspects.
      \end{itemize}
  
    \end{itemize}
    \item \textbf{Mohammad Mohsin Khan:}
    \begin{itemize}
      \item \textbf{Dynamic Testing Knowledge}  
      \begin{itemize}
          \item \textbf{Chosen Approach}: Hands-on practice with sample projects
          \item \textbf{Reason for Choice}: Practicing dynamic testing directly on projects will allow me to apply testing techniques in real-world scenarios, strengthening my ability to identify issues as they arise. This hands-on approach will provide practical insights that go beyond theoretical learning and help me become proficient in testing under realistic conditions.
      \end{itemize}
  
      \item \textbf{Performance Testing Knowledge}  
      \begin{itemize}
          \item \textbf{Chosen Approach}: Tool-specific training
          \item \textbf{Reason for Choice}: Tool-specific training allows me to gain direct experience with the performance testing tools that I will use in the project. This approach is ideal because it provides both a solid understanding of tool features and the technical skills needed to set up and execute performance tests effectively.
      \end{itemize}
  
    \end{itemize}
    \item \textbf{Luigi Quattrociocchi:}
    \begin{itemize}
        \item \textbf{Static Testing Knowledge}
        \begin{itemize}
            \item \textbf{Chosen Approach}: Tool-Specific Documentation and Tutorials
            \item \textbf{Reason for Choice}: Reading and following documentation and tutorials specific to static testing tools, such as ESLint, should allow me to gain a more detailed understanding of how these tools work. This approach will also make it easier for me to learn best practices for effective code analysis within the actual tool environment we'll be using.
        \end{itemize}
        
        \item \textbf{Test Case Management}
        \begin{itemize}
            \item \textbf{Chosen Approach}: Exploring Test Management Tools
            \item \textbf{Reason for Choice}: Familiarizing myself with test management tools should provide me with direct experience in organizing, creating, and tracking test cases. This is especially useful for our project because having a structured approach to documentation should streamline collaboration with other team members as we conduct testing.
        \end{itemize}
    \end{itemize}

\end{itemize}
\end{enumerate}

\end{document}