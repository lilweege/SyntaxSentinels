\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
November 1 & 1.0 & Initial documentation\\
% Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

% This document ... \wss{provide an introductory blurb and roadmap of the
%   Verification and Validation plan}
This document outlines the strategies and processes used to ensure that the plagiarism detection system  developed by the SyntaxSentinals team meets all functional 
and non-functional requirements. The primary goal of this plan is to build confidence in the correctness, usability, and performance of the system. It also focuses 
on identifying and mitigating potential risks, ensuring that the final product aligns with academic and competition standards.
This document is organized as follows. The general information section provides an overview of the objectives, challenges, and relevant project documents 
used throughout the V\&V process. The plan section describes the roles and responsibilities of the team members and the tools used for automated testing and verification. 
The system tests section lists the tests performed for both functional and non-functional requirements, with traceability to the SRS. 
The unit test description section details the unit testing scope, the modules tested, and the strategies for covering edge cases. Finally, the appendix contains symbolic parameters, 
survey questions (if applicable), and any other relevant information to support the V\&V process. This plan will evolve as the project progresses, with updates following the completion 
of the detailed design and implementation phases.

\section{General Information}

\subsection{Summary}

% \wss{Say what software is being tested.  Give its name and a brief overview of
%   its general functions.}
  The software being tested is a plagiarism detection system designed to identify similarities in Python code submissions, called SyntaxSentinels. 
  This system utilizes natural language processing (NLP) techniques to analyze code semantics, preventing common circumvention methods 
  like adding benign lines or altering variable names. Its core function is to allow users to input code snippets and receive a plagiarism 
  report containing similarity scores, which, when compared to a threshold, indicate the likelihood of plagiarism. This tool is primarily 
  intended for use in academic and competitive environments to promote fairness and integrity in code submissions.

\subsection{Objectives}

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}
The primary objectives of this V\&V plan are to:
\begin{itemize}
  \item Build confidence in the programs correctness by ensuring alignment with the SRS requirements.
  \item Show that we have met the documented safety and security requirements (SR-SAF1- SR-SAF5) in the Hazard Analysis document.
  \item Demonstrate adequate usability in the program by conducting functional and non-functional tests mentioned in this document.
\end{itemize}

Out of Scope:
\begin{itemize}
  \item Validation of any external libraries will be assumed to be handled by their maintainers.
\end{itemize}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

\subsection{Challenge Level and Extras}

% \wss{State the challenge level (advanced, general, basic) for your project.
% Your challenge level should exactly match what is included in your problem
% statement.  This should be the challenge level agreed on between you and the
% course instructor.  You can use a pull request to update your challenge level
% (in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
% VnV planning exercise.}

% \wss{Summarize the extras (if any) that were tackled by this project.  Extras
% can include usability testing, code walkthroughs, user documentation, formal
% proof, GenderMag personas, Design Thinking, etc.  Extras should have already
% been approved by the course instructor as included in your problem statement.
% You can use a pull request to update your extras (in TeamComposition.csv or
% Repos.csv) if your plan changes as a result of the VnV planning exercise.}

This project has been classified as having a General difficulty level, as agreed with the course instructor. 
Planned extras include:
\begin{itemize}
  \item A user manual for instructors and administrators.
  \item Benchmarking of the tool’s effectiveness compared to MOSS.
\end{itemize}

\subsection{Relevant Documentation}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.  You can create BibTeX entries for your
%   documents and within those entries include a hyperlink to the documents.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

The following documents are critical to the development and V\&V efforts for this project.

\begin{itemize}
    \item \textbf{Software Requirements Specification (SRS)}: Defines the project’s requirements, guiding both verification and validation. ~\citep{SRS}.
    \item \textbf{User Guide}: Provides operational instructions, relevant for usability testing. ~\citep{UserGuide}.
    \item \textbf{Module Guide (MG)}: Outlines the system’s architecture, essential for design verification. ~\citep{MG}.
    \item \textbf{Module Interface Specification (MIS)}: Details the internal modules and interfaces, critical for unit testing. ~\citep{MIS}.
    \item \textbf{Hazard Analysis}: Identifies potential risks, guiding validation efforts for safety and security. ~\citep{HazardAnalysis}.
\end{itemize}

\section{Plan}

This section outlines the structured plan for verification and validation of the project. It defines team roles, describes approaches for verifying different phases of the development process, and specifies tools to be used in ensuring the project meets quality standards. The subsections cover SRS verification, design verification, implementation verification, automated testing tools, and software validation.

\subsection{Verification and Validation Team}

The following table lists team members and their respective roles in the verification and validation process. Each member will be responsible for writing test cases, executing tests, and documenting results for their assigned areas. The team will meet regularly to discuss progress, address issues, and ensure alignment with project goals.

\begin{table}[h!]
  \centering
  \begin{tabular}{|p{5cm}|p{10cm}|}
      \hline
      \textbf{Team Member} & \textbf{Role in Verification and Validation} \\
      \hline
      Mohammad Mohsin Khan & Oversees system architecture verification and leads checklist creation. \\
      \hline
      Lucas Chen & Security verification, focusing on access control and data flow assessments. \\
      \hline
      Dennis Fong & Responsible for interface and compatibility reviews between components. \\
      \hline
      Julian Cecchini & Ensuring that individual modules or components integrate smoothly with one another. \\
      \hline
      Luigi Quattrociocchi & Tracks checklist items and maintains verification documentation. \\
      \hline
  \end{tabular}
  \caption{Verification and Validation Team Roles}
  \label{tab:team-roles}
\end{table}


\subsection{SRS Verification Plan}

The SRS (Software Requirements Specification) verification will ensure that all functional and non-functional requirements are accurately documented and align with the project goals. The verification plan includes:

\begin{itemize}
    \item \textbf{Structured Reviews}: Team members will perform a structured review of the SRS document, verifying that all requirements are feasible and testable.
    \item \textbf{Checklist-Based Verification}: An SRS checklist will be used to ensure all critical elements, such as functional completeness and clarity, are covered.
    \item \textbf{Reviewer Feedback Sessions}: We will gather feedback from peer reviewers and our project supervisor, meeting to discuss any discrepancies or ambiguous requirements. These meetings will include task-based inspections, where reviewers are asked to analyze requirements based on specific scenarios.
\end{itemize}

\pagebreak

The checklist for SRS review will cover:

\begin{center}
  \begin{longtable}{|p{4cm}|p{11cm}|}
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endfirsthead
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endhead
  
  \textbf{1. Completeness} & \\
  \hline
  1.1 Purpose and Scope & Document states the purpose and scope of the project. \\
  \hline
  1.2 Stakeholders & Key stakeholders (clients, users) are defined and are relevant. \\
  \hline
  1.3 Functional Requirements & All primary functions (e.g., plagiarism detection, reporting) are covered and are descriptive. \\
  \hline
  1.4 Non-Functional Requirements & Includes performance, usability, and security requirements. \\
  \hline
  \textbf{2. Clarity} & \\
  \hline
  2.1 Unambiguous Terminology & Each requirement is clearly stated, terms are defined (e.g., MOSS, NLP). \\
  \hline
  2.2 Glossary Completeness & All acronyms and terms are included in the glossary. \\
  \hline
  \textbf{3. Consistency} & \\
  \hline
  3.1 Consistent Terminology & Terminology and references are consistent throughout. \\
  \hline
  3.2 No Conflicting Requirements & No contradictory requirements (e.g., conflicting performance vs. security). \\
  \hline
  \textbf{4. Verifiability} & \\
  \hline
  4.1 Testable Requirements & Each requirement is testable and verifiable (e.g., accuracy metrics, response times). \\
  \hline
  4.2 Acceptance Criteria & Clear acceptance criteria for each requirement. \\
  \hline
  \textbf{5. Traceability} & \\
  \hline
  5.1 Unique Identifiers & Each requirement has a unique identifier. \\
  \hline
  5.2 Source of Requirements & Requirements link to stakeholder needs or project goals. \\
  \hline
  \textbf{6. Feasibility} & \\
  \hline
  6.1 Technical Feasibility & Requirements are achievable within project constraints. \\
  \hline
  6.2 Practical Constraints & Constraints such as budget and timeline are realistic. \\
  \hline
  \textbf{7. Security and Privacy} & \\
  \hline
  7.1 Data Retention Policy & Compliance with data privacy laws (e.g., PIPEDA). \\
  \hline
  7.2 Access Control Requirements & Requirements for user authentication and authorization are clear. \\
  \hline
  \textbf{8. Modifiability} & \\
  \hline
  8.1 Organized Structure & Requirements are logically organized for easy updates. \\
  \hline
  8.2 No Redundancies & No duplicate requirements to avoid confusion. \\
  \hline
  \textbf{9. Compliance and Ethics} & \\
  \hline
  9.1 Legal and Ethical Standards & Legal (e.g., Copyright) and ethical considerations are addressed. \\
  \hline
  \caption{Checklist for SRS Verification}
  \end{longtable}
  \end{center}

\subsection{Design Verification Plan}

The design verification plan aims to ensure that the design accurately implements the requirements and adheres to best practices. This plan includes:

\begin{itemize}
    \item \textbf{Checklist-Based Design Review}: A checklist will be used to guide reviews, focusing on key aspects such as modularity, scalability, and security.
    \item \textbf{Peer Design Reviews}: Peer reviews by classmates and team members will provide feedback on the design, highlighting potential areas of improvement.
    \item \textbf{Regular Team Reviews}: Scheduled meetings will allow the team to discuss any design modifications, verify alignment with the SRS, and ensure consistency across components.
\end{itemize}

The checklist for design review will cover:
\begin{center}
  \begin{longtable}{|p{4cm}|p{11cm}|}
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endfirsthead
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endhead
  
  \textbf{1. Functional Verification} & \\
  \hline
  1.1 Requirement Mapping & Each design element corresponds to at least one requirement in the SRS. \\
  \hline
  1.2 Functionality Coverage & The design covers all specified functionalities, including error handling and edge cases. \\
  \hline
  1.3 Interface Definition & All interfaces between components are clearly defined and consistent with requirements. \\
  \hline
  
  \textbf{2. Structural Verification} & \\
  \hline
  2.1 Modular Design & The design is divided into logical, independent modules with well-defined interfaces. \\
  \hline
  2.2 Dependency Analysis & Dependencies between components are minimized, and unnecessary couplings are avoided. \\
  \hline
  2.3 Hierarchical Structure & The design follows a clear hierarchy, with higher-level components orchestrating lower-level ones. \\
  \hline
  
  \textbf{3. Usability and Accessibility} & \\
  \hline
  3.1 User Interface Design & UI elements are consistent, intuitive, and meet accessibility standards (if applicable). \\
  \hline
  3.2 Navigation and Flow & User navigation and workflow are logical, efficient, and follow a coherent path. \\
  \hline
  3.3 Accessibility Standards & The design adheres to relevant accessibility guidelines, such as WCAG, to ensure usability for all users. \\
  \hline
  
  \textbf{4. Performance and Optimization} & \\
  \hline
  4.1 Performance Criteria & The design incorporates mechanisms to meet performance requirements (e.g., response time, resource usage). \\
  \hline
  4.2 Scalability & Design supports scalability to handle expected load increases without significant degradation. \\
  \hline
  
  \textbf{5. Security and Privacy} & \\
  \hline
  5.1 Data Flow Security & The design ensures secure data handling, storage, and transmission between components. \\
  \hline
  5.2 Access Control Mechanisms & Roles and permissions are implemented to restrict unauthorized access to sensitive components. \\
  \hline
  5.3 Compliance & Design complies with security and privacy standards as outlined in the SRS. \\
  \hline
  
  \textbf{6. Traceability and Documentation} & \\
  \hline
  6.2 Documentation Completeness & Documentation is complete, with descriptions of components, workflows, and data flow. \\
  \hline
  6.3 Version Control & Design documentation is version-controlled to track changes and updates. \\
  \hline
  
  \end{longtable}
  \end{center}

\subsection{Verification and Validation Plan Verification Plan}

The verification and validation plan itself will also undergo verification to ensure its effectiveness. This will be achieved by:

\begin{itemize}
    \item \textbf{Peer Reviews}: Classmates will review the plan to provide feedback on its clarity, feasibility, and alignment with project requirements.
    \item \textbf{Mutation Testing}: We will apply mutation testing to validate that the plan can effectively catch errors and discrepancies in project requirements and implementation.
    \item \textbf{Checklist-Based Verification}: A checklist specific to the verification and validation plan will guide reviewers in assessing all critical aspects of the plan.
\end{itemize}

\begin{center}
  \begin{longtable}{|>{\centering\arraybackslash}p{4cm}|p{10cm}|}
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endfirsthead
  
  \hline
  \textbf{Item} & \textbf{Description} \\
  \hline
  \endhead
  
  1. Verification Plan Completeness & Verify that the Verification and Validation Plan includes all necessary sections, such as objectives, scope, and methodologies. \\
  \hline
  2. Clarity of Objectives & Ensure that the objectives of the verification and validation activities are clearly stated and aligned with project goals. \\
  \hline
  3. Methodology Definition & Check that each verification method (e.g., reviews, inspections, testing) is well-defined with clear procedures. \\
  \hline
  4. Team Roles and Responsibilities & Confirm that each team member's role in the verification and validation activities is documented and clear. \\
  \hline
  5. Review and Inspection Procedures & Validate that there are structured procedures for design reviews and inspections, with criteria for passing/failing. \\
  \hline
  6. Integration of Automated Tools & Verify that automated tools for testing and validation (e.g., CI/CD, linters, static analyzers) are specified and included in the plan. \\
  \hline
  7. Traceability of Requirements & Confirm that the verification and validation activities trace back to specific project requirements to ensure coverage. \\
  \hline
  8. Acceptance Criteria & Ensure there are clear acceptance criteria defined for each verification and validation task. \\
  \hline
  9. Documentation of Test Cases & Verify that each planned test case has clear documentation, including expected outcomes, inputs, and procedures. \\
  \hline
  10. Risk Management in Verification & Confirm that potential risks in the verification and validation process are identified and mitigation strategies are documented. \\
  \hline
  11. Feedback Loop & Ensure that there is a mechanism for capturing feedback and iterating on the verification and validation process as needed. \\
  \hline
  12. Reporting and Tracking of Issues & Check that there is a process for documenting, tracking, and addressing issues found during verification and validation. \\
  \hline
  13. Schedule and Milestones & Verify that there is a realistic schedule and milestones for completing verification and validation activities. \\
  \hline
  
  \end{longtable}
  \end{center}

\subsection{Implementation Verification Plan}

The implementation verification plan focuses on ensuring the correctness and quality of the implementation phase. The plan includes:

\begin{itemize}
    \item \textbf{Unit Testing}: Each function and module will undergo unit testing using the frameworks mentioned in section 3.6 to ensure they meet functional requirements.
    \item \textbf{Static Code Analysis}: We will use static analysis tools mentioned in section 3.6 to verify code quality, adherence to coding standards, and security practices.
    \item \textbf{Code Walkthroughs}: Code walkthroughs will be held in team meetings and during the final presentation, allowing team members to inspect each other’s code for issues in logic, structure, and readability.
\end{itemize}

\subsection{Automated Testing and Verification Tools}

To streamline the verification process, the following automated testing and verification tools will be used:

\begin{itemize}
  \item \textbf{Testing Framework}: A testing framework (Pytest for Python) and (Playwright for ReactJs) will be used to automate testing of individual functions, modules, and E2E.
  \item \textbf{Continuous Integration (CI) Tool}: GitHub Actions will be set up for continuous integration to ensure that tests are automatically run on new code submissions.
  \item \textbf{Code Coverage Tool}: Code coverage tools (e.g Pytest with Coverage.py) will track the extent to which the codebase has been tested, ensuring that all critical paths are covered.
  \item \textbf{Linters/Static Analysis Tools}: Linters appropriate to the project’s programming language will enforce coding standards, improving code readability and maintainability.
\end{itemize}

\subsection{Software Validation Plan}

The software validation plan outlines the strategies for validating that the software meets the intended requirements. This includes:

\begin{itemize}
    \item \textbf{User Review Sessions}: Review sessions with stakeholders and user representatives will be conducted to validate that the system meets user needs and expectations.
    \item \textbf{Rev 0 Demonstration}: Shortly after the scheduled Rev 0 demo, we will seek feedback from stakeholders and supervisors, if applicable, to confirm that the design and initial implementation align with project goals.
    \item \textbf{End to End Testing}: We will plan E2E sessions to ensure the software works correctly from start to end and meets all functional requirements.
    \item \textbf{Performance Testing}: Once functional requirements have been implemented, non-functional requirements will be implemented then performance testing will be conducted to validate that the software meets non-functional requirements, such as response times and resource usage.
\end{itemize}

The validation process will involve gathering external data, where possible, to test the system’s accuracy and performance under realistic scenarios.

\section*{Summary}

This comprehensive plan for verification and validation addresses each phase of the project lifecycle, from requirements to implementation. By following a structured approach with well-defined roles, checklists, and automated tools, we aim to ensure a high level of quality, accuracy, and security in the final product.

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Look and Feel Requirements}

\subsubsection{Usability and Humanity Requirements}

\subsubsection{Performance Requirements}

\paragraph{Batch processing time is less than 10 minutes}

\begin{enumerate}
  \item{Capacity-Test-1}
  
  NFR: PR-C1

  Type: Dynamic

  Initial State: The system is idle and ready to process a batch of code submissions

  Input:Submit a batch of inputs (code submissions) to the system for processing.

  Output: The system completes processing the batch within 600 seconds (10 minutes).

  How test will be performed:

  \begin{enumerate}
    \item Submit a batch of code submissions to the system.
    \item Measure the time taken for the system to process the batch.
    \item Verify that the processing time is less than 600 seconds.
  \end{enumerate}
\end{enumerate}

\paragraph{Notification is sent when Processing Time Exceeds Ten Minutes}
\begin{enumerate}
  \item{Speed-Latency-Test-1}

  NFR: PR-SL2

  Type: Performance, Manual
            
  Initial State: System is idle; no processes are currently running.
            
  Input/Condition: Start a task that is expected to run for more than ten minutes.
            
  Output/Result: An email is sent to the user once processing exceeds ten minutes.
            
  How test will be performed: 
  \begin{enumerate}
    \item Start a process that is known to take more than ten minutes to complete. For example, a large number of files.
    \item Wait for 10 minutes while the process is running.
    \item Check the user's email inbox for the notification and verify it was received exactly 10 minutes after the process started.
    \item Check that the notification for correctness and that it is clear and understandable for the user.
  \end{enumerate}

  \item{Speed-Latency-Test-2}
  NFR: PR-SL2

  Type: Performance, Automated
            
  Initial State: System is idle; no processes are currently running.
            
  Input: Initiate a new task that is simulated to take over ten minutes.
            
  Output: Automated system log entry indicating that a user notification was triggered after ten minutes.
            
  How test will be performed: 
  \begin{enumerate}
    \item {Setup a mock process to run for over 10 minutes.}
    \item {Verify after 10 minutes were up wether a function call was made to send the notification.}
    \item {Automatically check system logs or notifications queue to verify a notification was generated after ten minutes.}
  \end{enumerate}
\end{enumerate}

\paragraph{System remains operational after malformed input}
\begin{enumerate}
  \item{Robustness-Test-1}

  NFR: PR-RFT1

  Type: Robustness, Dynamic
            
  Initial State: System is operational, awaiting user input.
            
  Input:  Malformed input (e.g., corrupted file) is submitted to the system.
            
  Output: System displays an error message to the user indicating invalid input, without crashing or becoming unresponsive.
            
  How test will be performed: 
  \begin{enumerate}
    \item navigate to the home page
    \item Submit a file that is known to be corrupted or invalid.
    \item Verify that an error message is displayed to the user by checking if the function call was made.
    \item Submit a file that is known to be valid.
    \item Verify that the system accepts the valid input and processes it correctly.
  \end{enumerate}
\end{enumerate}

\paragraph{System is able to interface with cloud computing services}
\begin{enumerate}
  \item{Adjacent-Systems-Test-1}

  NFR: OE-IAS1

  Type: Manual
            
  Initial State: System setup with no cloud service connected.
            
  Input:  User attempts to configure and connect the system to a cloud service (e.g. AWS or Azure).
            
  Output: System successfully connects to the selected cloud service and displays a confirmation message.
            
  How test will be performed: 
  \begin{enumerate}
    \item navigate to the cloud configuration settings.
    \item Select a cloud provider (AWS or Azure) and enter necessary credentials.
    \item Verify that the system displays a confirmation message upon successful connection.
  \end{enumerate}
\end{enumerate}

\paragraph{System is deployable on Free Hosting Service}
\begin{enumerate}
  \item{Adjacent-Systems-Test-2}

  NFR: OE-IAS2

  Type: Manual
            
  Initial State: User clones the system repository from GitHub.
            
  Input:  Deploy the system on a free hosting service (e.g. Heroku, Netlify).
            
  Output: System is accessible via a public URL and functions as expected.
            
  How test will be performed: 
  \begin{enumerate}
    \item Clone the system repository from GitHub.
    \item Deploy the system on a free hosting service.
    \item Verify that the system is accessible via a public URL and functions as expected without any issues.
  \end{enumerate}
\end{enumerate}

\paragraph{System is able to authenticate the user via external services}
\begin{enumerate}
  \item{Adjacent-Systems-Test-3}

  NFR: OE-IAS3

  Type: Manual
            
  Initial State: User is not authenticated and does not have a valid auth token.
            
  Input: User attempts to log in using an external service (e.g. Google, GitHub).
            
  Output: System authenticates the user and grants access to the UI.
            
  How test will be performed: 
  \begin{enumerate}
    \item Navigate to the login page.
    \item Select an external service (e.g. Google, GitHub) to log in with.
    \item Verify that the system authenticates the user and is redirected to the home page.
  \end{enumerate}
\end{enumerate}

\subsubsection{Operational and Environmental Requirements}

\subsubsection{Maintainability and Support Requirements}

\paragraph{Model release is accompanied by a report of metrics}

\begin{enumerate}

  \item{Maintaince-Test-2}

  NFR: MS-M2

  Type: Manual

  Initial State: The model has been trained and is ready for release

  Input: Release the model

  Output: A report is generated with metrics such as accuracy, precision, etc.

  How test will be performed:
  Release the model and verify that a report is generated with relevant metrics. A verifiable metric is that the report exists.
  This test will be done manually by the team members of SyntaxSentinels.

\end{enumerate}

\paragraph{A pathway for users to post or vote for requests/issues}

\begin{enumerate}
  \item{Maintaince-Test-3}

  NFR: MS-S3

  Type: Manual

  Initial State: The user has a GitHub account and is logged in

  Input: User posts a request or issue

  Output: The request or issue is posted and visible to other users

  How test will be performed:
  The user will post a request or issue and verify that it is visible to other users. A verifiable metric is that the request or issue is visible to other users.
  This test will be done manually by the team members of SyntaxSentinels.
\end{enumerate}
\subsubsection{Security Requirements}

\paragraph{User can access UI with valid login credentials}
\begin{enumerate}
  \item{Security-Test-1\\}

  NFR: SR-A1
  Type: Dynamic
            
  Initial State: The user is not authenticated and does not have a valid auth token.
            
  Input/Condition: User enters valid login credentials (username and password).
            
  Output/Result: The user is authenticated and gains access to the UI.
            
  How test will be performed: 
  This test will be done via UI automated test suite using Playwright. The test will simulate a user entering valid login 
  credentials and verify that the user is authenticated and gains access to the UI.
\end{enumerate}
\paragraph{User can access API with valid auth token}
\begin{enumerate}
  \item{Security-Test-2\\}

  NFR: SR-A1

  Type: Dynamic
            
  Initial State: The user is not authenticated and does not have a valid auth token
            
  Input: User attempts to access the API with a valid auth token
            
  Output: The API allows the user to upload code for comparison
            
  How test will be performed: 
  Pass a valid auth token to the API and verify that the system allows code upload and returns a success response.

  \item{Security-Test-3}

  NFR: SR-A1

  Type: Dynamic

  Initial State: The user is not authenticated and does not have a valid auth token

  Input: User attempts to access the API with an invalid or missing auth token

  Output: The API denies access and returns an unauthorized response

  How test will be performed:
  Pass an invalid or missing auth token to the API and verify that the system denies access and returns an unauthorized response.

\end{enumerate}

\subsubsection{Cultural Requirements}

\subsubsection{Compliance Requirements}

\subsubsection{Area of Testing2}
...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}
This section will not be filled in until after the MIS document has been completed.
% \wss{This section should not be filled in until after the MIS (detailed design
%   document) has been completed.}

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}  

% \wss{To save space and time, it may be an option to provide less detail in this section.  
% For the unit tests you can potentially layout your testing strategy here.  That is, you 
% can explain how tests will be selected for each module.  For instance, your test building 
% approach could be test cases for each access program, including one test for normal behaviour 
% and as many tests as needed for edge cases.  Rather than create the details of the input 
% and output here, you could point to the unit testing code.  For this to work, you code 
% needs to be well-documented, with meaningful names for all of the tests.}

% \subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 
					
% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 

% \item{...\\}
    
% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}
		
% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input/Condition: 
					
% Output/Result: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

% \wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 

  While writing this deliverable, our team was able to collaboratively clarify and solidify 
  our understanding of both the functional and non-functional requirements. This process helped us align on 
  specific goals and create comprehensive test plans, which ensures that our testing will effectively verify 
  that all key project requirements are met.

  \item What pain points did you experience during this deliverable, and how
    did you resolve them?

  A significant challenge we faced was the extensive amount of non-functional requirements (NFRs), 
  with over 30 NFRs to address. Each NFR required us to develop a detailed test plan, specifying 
  factors like type (e.g., functional, dynamic, manual), initial state, input/conditions, expected 
  output/results, and test method. While the template helped streamline our process, the sheer volume 
  of NFRs meant the documentation grew quickly, and managing this without sacrificing detail was challenging. 
  We prioritized efficiency by dividing NFRs among team members and holding review sessions to ensure consistent 
  quality and adherence to our testing criteria. This allowed us to maintain clarity without being overwhelmed by 
  the documentation demands.
  One of the main challenges we faced was the large amount of 
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \begin{itemize}
    \item \textbf{Dynamic testing knowledge}
    \item \textbf{Static testing knowledge}
    \item \textbf{Automated testing tools}
    \item \textbf{Security testing knowledge}
    \item \textbf{Performance testing knowledge}
    \item \textbf{Test Case Management}
  \end{itemize}

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
  \begin{itemize}
    \item \textbf{Dynamic Testing Knowledge}
    \begin{itemize}
        \item \textbf{Online Courses and Tutorials}: We can learn dynamic testing techniques through structured courses on platforms like Udemy, Coursera, or LinkedIn Learning, which cover both functional and non-functional testing strategies.
        \item \textbf{Hands-on Practice with Sample Projects}: Practicing dynamic testing on sample projects or real scenarios allows us to directly apply the techniques, reinforcing our understanding through application.
    \end{itemize}

    \item \textbf{Static Testing Knowledge}
    \begin{itemize}
        \item \textbf{Tool-Specific Documentation and Tutorials}: Reading the documentation and tutorials for static testing tools (e.g. ESLint) helps us understand how to perform effective static code analysis.
        \item \textbf{Webinars and Workshops}: Many companies offer webinars or workshops on static analysis, often with interactive demos that can help us learn the nuances of static testing.
    \end{itemize}

    \item \textbf{Automated Testing Tools}
    \begin{itemize}
        \item \textbf{Training and Certification Programs}: Courses that cover automation tools (like Pytest) and their integration with CI/CD pipelines provide a solid foundation for us in automated testing.
        \item \textbf{Building a Project with CI/CD Integration}: By implementing automated testing on a project using CI/CD tools like Jenkins or GitHub Actions, we can apply automation skills in real scenarios, enhancing both our tool knowledge and integration capabilities.
    \end{itemize}

    \item \textbf{Security Testing Knowledge}
    \begin{itemize}
        \item \textbf{Cybersecurity and Penetration Testing Courses}: Taking courses on platforms like Cybrary or Coursera offers insights into security testing practices, covering areas like penetration testing, vulnerability assessment, and secure code practices.
        \item \textbf{Practice with Security Testing Tools}: We can use tools like OWASP ZAP or Burp Suite for hands-on security testing, which helps us identify vulnerabilities and ensure code security.
    \end{itemize}

    \item \textbf{Performance Testing Knowledge}
    \begin{itemize}
        \item \textbf{Tool-Specific Training (e.g., JMeter, Locust)}: Learning a performance testing tool through its official documentation, tutorials, or community guides helps us understand load testing, scalability testing, and performance profiling.
        \item \textbf{Performance Testing Workshops or Certifications}: Many organizations provide certifications or workshops focused on performance testing methodologies and tool usage, which provide both theoretical knowledge and practical applications for us.
    \end{itemize}

    \item \textbf{Test Case Management}
    \begin{itemize}
        \item \textbf{Exploring Test Management Tools}: Familiarizing ourselves with tools like TestRail, Zephyr, or Jira for creating, organizing, and tracking test cases helps us manage our testing process efficiently.
        \item \textbf{Learning Best Practices in Test Documentation}: By reading resources or taking tutorials on effective test case design and management, we can develop a structured, comprehensive approach to test case documentation.
    \end{itemize}

\end{itemize}
  \begin{itemize}
    \item \textbf{Lucas Chen:} 
    \item \textbf{Dennis Fong:}
    \item \textbf{Julian Cecchini:} 
    \item \textbf{Mohammad Mohsin Khan:}
    \item \begin{itemize}
      \item \textbf{Dynamic Testing Knowledge}  
      \begin{itemize}
          \item \textbf{Chosen Approach}: Hands-on practice with sample projects
          \item \textbf{Reason for Choice}: Practicing dynamic testing directly on projects will allow me to apply testing techniques in real-world scenarios, strengthening my ability to identify issues as they arise. This hands-on approach will provide practical insights that go beyond theoretical learning and help me become proficient in testing under realistic conditions.
      \end{itemize}
  
      \item \textbf{Performance Testing Knowledge}  
      \begin{itemize}
          \item \textbf{Chosen Approach}: Tool-specific training
          \item \textbf{Reason for Choice}: Tool-specific training allows me to gain direct experience with the performance testing tools that I will use in the project. This approach is ideal because it provides both a solid understanding of tool features and the technical skills needed to set up and execute performance tests effectively.
      \end{itemize}
  
  \end{itemize}
    \item \textbf{Luigi Quattrociocchi:}
\end{itemize}
\end{enumerate}

\end{document}